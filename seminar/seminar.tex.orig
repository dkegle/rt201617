\documentclass[12pt,a4paper]{amsart}
\usepackage[slovene, english]{babel}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[dvipsnames,usenames]{color}
\usepackage{algorithmicx,algpseudocode}
\usepackage{graphicx}
\usepackage{hyperref}


\textwidth 15cm
\textheight 24cm
\oddsidemargin.5cm
\evensidemargin.5cm
\topmargin-5mm
\addtolength{\footskip}{10pt}
\pagestyle{plain}

\overfullrule=15pt % oznaci predlogo vrstico

\newtheorem{definition}{Definition}[section]
\newtheorem{lemma}[definition]{Lemma}
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{corollary}[definition]{Corollary}

\def\R{\mathbb R}
\def\N{\mathbb N}
\def\Z{\mathbb Z}
\def\C{\mathbb C}
\def\Q{\mathbb Q}

\begin{document}

\thispagestyle{empty}
\noindent{\large
University of Ljubljana \hfill  \today\\[1mm]
Faculty of Computer and Information Science  \\[5mm]
%IŠRM -- 2.~stopnja
}
%\vfill
\begin{center}{\large
Computational topology\\[4mm]
% Seminarska naloga\\[4mm]
{\bf Text classification using persistent homology}\\[4mm]
Matija Čufar, Domen Keglevič\\[6mm]
}
\end{center}
\bigskip

\section{Introduction}
Persistent homology is a method from topological data analysis which enables us to calculate and 
compare features of topological spaces at different spatial resolutions. Some features persist 
at wide range of spatial resolutions and are more likely to represent the underlying space. This 
approach  has been applied to many different domains ranging from image analysis
 (\textcolor{red}{bib [2]}) to
  genome studies (\textcolor{red}{bib [3]}).

 In this seminar assignment we try to use persistence homology for text classification. We pick four 
 different text domains and try to 
distinguish them based on their persistence diagrams.

\section{Methods}

We have chosen texts from the following domains:

\begin{itemize}
  \setlength\itemsep{0.5em}
\item Excerpts from the Old and New Testaments of the Bible,
\item articles from \url{phys.org},
\item recipes from \url{allrecipes.com}.
\end{itemize}


For each of the domains, we picked ten texts, each at least $100$ words long. We
used the Gudhi~\cite{maria2014gudhi}, a library for topological data analysis,
to compute persistent homology on the texts.

To compute the persistent homology of a data set, we first need to represent it
as a simplicial, or some other kind of complex. We used the following two
approaches to build simplicial complexes for each of the domains:

\subsection{Feature based approach}

In our first approach we associated each text with a point in $\R ^6$. Each dimension was representing one of text features. We used the following features:
\begin{itemize}
  \setlength\itemsep{0.5em}
  \item the ratio of (average word length)/(longest word length),
  \item the ratio of (average sentence length)/(longest sentence length),
  \item the ratio of the total number of three words with the highest tf-idf
    value among all the words,
  \item the ratio of the number of words of length $\le 8$ among all words,
<<<<<<< HEAD
  \item the ratio of the number of words of length $\ge 9$ among all words,
  \item the ratio of (number of different words)/(number of all words).
\end{itemize}

This gives us a point in $\R^6$ for each of the texts. We used these points to
build Alpha and Vietoris-Rips complexes on each of the domains.
=======
  \item the ratio of the number of words of length $\ge 9$ among all words.
  \item the ratio of (number of different words)/(number of all words)
\end{itemize}

Note that all features are normalized in order to avoid giving too much weight 
to those that would have large size otherwise.
After computing features and mapping texts to points in $\R ^6$ we
built Alpha and Vietoris-Rips complexes on each of the domains.
>>>>>>> 56019df42d6d2beecb6ca3725f19de9b480bd7fe

\subsection{Distribution based approach}

Our second approach involved computing distributions of word and sentence
lengths. This gives us two distributions for each text. Two distributions of the same type (word or sentence) can then be compared using the following distance measures:

\begin{itemize}
\item The Hellinger distance:

\begin{equation*}
  H(P,Q) = \sqrt{\frac{1}{2} \sum_{i=1}^k\left(\sqrt{p_i} -
    \sqrt{q_i}\right)^2}\ ,
\end{equation*}

\item the Chi-squared distance:

\begin{equation*}
  \chi^2(P,Q) = \frac{1}{2}\sum _{i=1}^n\frac{(p_i - q_i)^2}{(p_i + q_i + \varepsilon)}\ ,
\end{equation*}

\item the Euclidean distance:

\begin{equation*}
  E(P,Q) = \sqrt{\sum_{i=1}^k\left(p_i - q_i\right)^2}\ ,
\end{equation*}
\end{itemize}

\noindent
where $P$ and $Q$ are the discrete distributions and $p_i$ and $q_i$ are the
$i$-th bins of corresponding distributions. The $\varepsilon$ in the Chi-squared
distance is a small constant used to avoid dividing by zero.

We used these distances to compute distance matrix for each of the
domains and used distance matrices to build Vietoris-Rips complexes.

\subsection{Domain comparison}

When the simplicial complexes were built, we calculated persistence diagrams for
each complex and computed the bottleneck distances between them. We expected the
distances would help us distinguish between the texts. For example, the distance
between the texts taken from the Bible should be smaller than the rest.

\section{Results}

\subsection{Feature based approach} Bottleneck
distance matrix for the Alpha complex is shown in Table~\ref{tab:alpha}. 
Barcode plots are shown in Figure~\ref{fig:barcode:alpha}.

As we can see from the distance matrix, the method did not produce meaningful
results. The first thing we notice is that all the distances are very small,
with the highest being only $0.008$. Other examples that show the ineffectiveness
of this method are the fact that the Old Testament differs the most from the New
Testament and the fact that according to this method, physics article abstracts
are very similar to recipes.

Something we notice from the barcode plots is that
\emph{\textcolor{red}{TODO}: napisat neki o 4d zadevah v receptih}

Results for the Vietoris-Rips complex are similar to the results for Alpha complex.

\begin{table}
  \centering
<<<<<<< HEAD
  \begin{tabular}{c|cccc}
=======
  \begin{tabular}{c|ccccc}
>>>>>>> 56019df42d6d2beecb6ca3725f19de9b480bd7fe
                  & Old Testament & New Testament & phys.org & recipes \\ \hline
    Old Testament & $0.000$ & $0.008$ & $0.003$ & $0.003$ \\
    New Testament & $0.008$ & $0.000$ & $0.008$ & $0.008$ \\
    phys.org      & $0.003$ & $0.008$ & $0.000$ & $0.002$ \\
    recipes       & $0.003$ & $0.008$ & $0.002$ & $0.000$ \\
  \end{tabular}

  \caption{The distance matrix calculated from the Alpha complexes.}
  \label{tab:alpha}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/bible-new-alpha}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/bible-old-alpha}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/phys-alpha}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/recipes-alpha}
  \caption{Persistence barcodes of the domains, calculated from the Alpha
    complexes.}
  \label{fig:barcode:alpha}
\end{figure}

\subsection{Distribution based approach}

\emph{\textcolor{red}{TODO}: tekst}

\begin{table}
  \centering
<<<<<<< HEAD
  \begin{tabular}{c|cccc}
=======
  \begin{tabular}{c|ccccc}
>>>>>>> 56019df42d6d2beecb6ca3725f19de9b480bd7fe
                  & Old Testament & New Testament & phys.org & recipes \\ \hline
    Old Testament & $0.000$ & $0.015$ & $0.040$ & $0.112$ \\
    New Testament & $0.015$ & $0.000$ & $0.033$ & $0.103$ \\
    phys.org      & $0.040$ & $0.033$ & $0.000$ & $0.105$ \\
    recipes       & $0.112$ & $0.103$ & $0.105$ & $0.000$ \\
  \end{tabular}

  \caption{The distance matrix calculated from the Vietoris-Rips complexes,
    built using the Hellinger distance between sentence length distribution.}
  \label{tab:hell}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/bible-new-hell}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/bible-old-hell}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/phys-hell}
  \includegraphics[width=0.45\textwidth]{../plots/barcodes/recipes-hell}
  \caption{Persistence barcodes of the domains, calculated from the
    Vietoris-Rips complexes, built using the Hellinger distance between sentence
    length distributions.}
  \label{fig:barcode:hell}
\end{figure}

\section{Conclusion}

As we saw in the previous section, the results are not very good.



\section{Authors contributions}
Both authors have contributed to most parts of the seminar work. The exception being that Domen prepared most of the presentation and Matija wrote most of the report. Detailed contributions can be seen in commit history of GitHub repository \url{https://github.com/dkegle/rt201617} which also contains all of the code and data.

% biblio
\bibliographystyle{plain}
\bibliography{biblio}

% [2] Image analysis: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4448533/
% [3] Genome-wide maps: http://www.cell.com/cell-systems/pdf/S2405-4712(16)30183-1.pdf

\end{document}
